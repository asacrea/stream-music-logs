# Solution: Building a Data Lake and Data Warehouse for Eventsim Streaming Data

### Introduction

This documentation outlines the solution for building a data lake and data warehouse for processing the streaming data generated by Eventsim, a program that simulates event data for a fake music website. The goal of this solution is to enable the BI team to perform analysis on the data and answer specific questions related to user activity on the website.

The solution will be designed to:

1. Fetch the streaming data from Eventsim and store it in a data lake.
2. Create tables with proper data index/partitions to facilitate data access.
3. Create transformed tables that facilitate analysis by the BI team.
4. Store the transformed data in a data warehouse that is SQL-queriable.

### Technical Assumptions

* Streaming Fetch Process: The fetching process of the event logs should be implemented as a streaming mechanism, consuming the data in real-time.
* Data Lake Resilience: The raw event log files stored in the data lake may disappear after processing. The solution should consider this possibility and accommodate future processing needs if required.
* Data Analysis: The specific analytical questions mentioned in the introduction (e.g., user locations, popular songs, music consumption hours) are not the primary focus of this exercise. The solution aims to provide a scalable infrastructure to enable such analysis but does not need to answer those specific questions.
* Local Development Containerization: If developing the solution locally, it should be containerized to ensure consistency and reproducibility. Containerization technologies like Docker can be utilized for this purpose.
* The work was divided into two folders, each with a different solution to the problem. The solution that was carried out to fulfill the purposes of this project is inside the "stream-music-logs-athena" folder.

### Solution Overview

The solution will leverage various AWS services to build a scalable and cost-effective data lake and data warehouse infrastructure. The architecture will include the following components:

1. Eventsim: The program that generates the streaming data for the fake music website.
2. Fluentd: A log data collector that tails the event logs from Eventsim and sends them as packages of JSON files to a storage solution.
3. Minio Cluster or S3: A storage solution where the event logs will be stored.
4. AWS Glue: A fully managed extract, transform, and load (ETL) service that will be used for data ingestion and transformation.
5. AWS Step Functions: A serverless workflow service that will orchestrate the data processing pipeline.
6. AWS Lambda: Serverless functions that will be used for custom data processing tasks.
7. AWS Kinesis Firehose: A fully managed service for ingesting and transforming streaming data.
8. AWS Athena as Database: The data warehouse that will store the transformed data and enable SQL queries for analysis.

### Solution Architecture

![Logo de Markdown](images/Cloud_Architechture.png)

The solution architecture consists of the following components and steps:

![Logo de Markdown](images/CloudServices.png)

1. Eventsim generates the streaming data, which is collected by Fluentd.

Let's start at the beginning, in order to run Eventsim successfully. First a couple of fixes had to be made to the Docker container.

The first problem was basically that the SDK that came by default to install from the Dockerfile, is not compatible with the architecture of the processor that Mac uses (M1 chip), which is based on an Arm64 architecture.

For this reason, the belsoft SDK was replaced by the belsoft liberica SDK.


```docker
# Install Java 8
RUN wget -q -O - https://download.bell-sw.com/pki/GPG-KEY-bellsoft |  apt-key add -
RUN echo "deb [arch=arm64] https://apt.bell-sw.com/ stable main" | tee /etc/apt/sources.list.d/bellsoft.list
RUN apt-get update
RUN apt-get install bellsoft-java8 -y
```

Running and building test scenarios using Eventsim was particularly difficult for me, as the container consumed too many resources to run some of the scenarios. Despite this, it was possible to simulate on a small scale the execution of Eventsim by making use of multi-threads multi-threads to have several generators at the same time sending logs. It was thanks to this ability to efficiently manage data from various information sources and send it to multiple destinations simultaneously.

 
```bash
#!/bin/bash

num_threads=5  # Number of threads to create

for ((i=0; i<num_threads; i++))
do
    nohup bash eventsim.sh -c "examples/example-config.json" --from 365 --nusers 1000 --growth-rate 0.01 ./output > eventsim.stoud &
done

wait  # Wait for all threads to finish
```


2. In the case of Minio, I decided to choose S3 as the data warehouse to run the tests, but not first without exploring Minio and getting a better understanding of how it interfaced with Eventsim and Fluend. For these it was necessary to make a small correction, which at least using Mac, had to be done. In the most stable version for Debian distributions, the credentials configuration changed from:

```bash
MINIO_ACCESS_KEY=minio_user
MINIO_SECRET_KEY=minio_pass
MINIO_REGION=us-east-1
```
To:

```bash
MINIO_ROOT_USER=minio_user
MINIO_ROOT_PASSWORD=minio_pass
MINIO_REGION=us-east-1
```

3. Fluentd sends the event logs as JSON files to a storage solution, either Minio Cluster or S3.

As I explored Fluentd in more detail, I understood that Fluentd is a flexible log collector that can collect logs from various sources, perform data transformations and route them to different destinations. However, if you send the data directly from Fluentd to an S3 bucket, you will be responsible for ensuring the reliability and scalability of the data pipeline. You will have to deal with potential problems such as network failures, retries, buffering and managing the scalability of the infrastructure. For these reasons I made the decision to make use of both tools and take advantage of the benefits of each, as Kinesis Firehose is a self-managed, self-scalable tool that makes it easier for us to deploy the solution and generate a more robust proposition.

To make the connection with S3, the existing connection with minio was replaced by adding a plugin that would allow this communication. The code to do this is as follows:

```XML
<source>
  @type tail
  <parse>
    @type json
  </parse>
  path /opt/eventsim/output/listen_events
  pos_file /var/log/td-agent/listen_events.pos
  tag kinesis.access
</source>

<match kinesis.**>
  @type kinesis_firehose
  aws_key_id AKIAUSRFN6A2SLQQ57QS         # The access key for Minio
  aws_sec_key ANR5UxYFE5DT6r1Ys4lwjQfIYELXg9WOKzEW7qVI        # The secret key for Minio
  delivery_stream_name PUT-S3-THtvd         # The bucket to store the log data
  region us-east-1           # See the region settings of your Minio server

<buffer time>
    @type file
    path /var/log/td-agent/s3
    timekey 60m                 # Flush the accumulated chunks every hour
    timekey_wait 1m             # Wait for 60 seconds before flushing
    timekey_use_utc true        # Use this option if you prefer UTC timestamps
    chunk_limit_size 3m       # The maximum size of each chunk
  </buffer>
</match>
```

4. AWS Glue is used to ingest the event logs from the storage solution and transform the data. For the development of this project, AWS Glue is used to explore the different states of the information within the data lake and thus generate a data catalogue that allows the exploration of information by means of SQL queries. On the other hand, AWS Glue allows us to run jobs to perform the transformation process of the information and ensure a better way of organising it to optimise the use of resources and to speed up the query execution process.


5. AWS Step Functions orchestrate the ETL workflow, coordinating the execution of various AWS Glue crawlers, Glue jobs and Lambda functions.

For this implementation, we proposed the development of a simple state machine to perform data exploration and catalogue generation for both raw data and data that underwent transformation by glue jobs. The Step Functions also allows us to monitor the state of the Crawlers and determine if it has finished its execution in order to continue with the flow towards the transformation of the data using the Glue Jobs.

![Logo de Markdown](images/StepFunction.png)


6. AWS Glue jobs perform the necessary transformations on the data, such as converting data types, adding derived columns, and partitioning the data.

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql.functions import col, to_date, date_format, substring, from_unixtime
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job


args = getResolvedOptions(sys.argv, ["JOB_NAME"])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args["JOB_NAME"], args)

# Script generated for node S3 bucket
music_table = glueContext.create_dynamic_frame.from_catalog(
    database="gluedb-0fbz600ygcnb",
    table_name="source",
    #transformation_ctx="S3bucket_node1",
)

#music_table.printSchema()

# Convert the int field to a date field
transformed_date = ApplyMapping.apply(
    frame=music_table,
    mappings=[
        ("artist", "string", "artist", "string"),
        ("song", "string", "song", "string"),
        ("duration", "double", "duration", "double"),
        ("ts", "long", "ts", "long"),
        ("sessionId", "int", "sessionId", "int"),
        ("auth", "string", "auth", "string"),
        ("level", "string", "level", "string"),
        ("itemInSession", "int", "itemInSession", "int"),
        ("city", "string", "city", "string"),
        ("zip", "string", "zip", "string"),
        ("state", "string", "state", "string"),
        ("userAgent", "string", "userAgent", "string"),
        ("lon", "double", "lon", "double"),
        ("lat", "double", "lat", "double"),
        ("userId", "int", "userId", "int"),
        ("lastName", "string", "lastName", "string"),
        ("firstName", "string", "firstName", "string"),
        ("gender", "string", "gender", "string"),
        ("registration", "long", "registration", "long"),
    ],
    transformation_ctx="transformed_date",
)

# Convert the dynamic frame back to a data frame
music_table_df = transformed_date.toDF()

#column_values = music_table_df.select("report_date").show()

music_table_df = music_table_df.withColumn('report_date', to_date(from_unixtime(music_table_df['ts'] / 1000, 'yyyy-MM-dd'))) \
                              .withColumn('registration_date', to_date(from_unixtime(music_table_df['registration'] / 1000, 'yyyy-MM-dd'))) \
                              .withColumn('year', date_format('report_date', 'yyyy')) \
                              .withColumn('month', date_format('report_date', 'MM')) \
                              .withColumn('day', date_format('report_date', 'dd'))

# Convert the data frame back to a dynamic frame
dfy = DynamicFrame.fromDF(dataframe=music_table_df, glue_ctx=glueContext, name="dfy")

datasink = glueContext.write_dynamic_frame.from_options(
    frame=dfy,
    connection_type="s3",
    connection_options={"path": "s3://music-stream-datalake/stage/",
                        "compression": "snappy",
                        "partitionKeys": ["year", "month", "day", "state", "city", "level", "gender"]},
    format="parquet",
    transformation_ctx="datasink"
)

job.commit()
```

7. AWS Glue writes the transformed data to a staging area in the data lake, using a partitioning scheme based on relevant attributes (e.g., year, month, day, state, city).

![Logo de Markdown](images/Index.png)

The following is the folder hierarchy generated by the glue job process, so that from this structure the execution of queries based on the partitions generated in the data catalog tables can be optimized

```python
s3://my-music-data/music_data_partitioned/
  ├── date=2023-07-01/
  │   ├── country=US/
  │   │   ├── state=CA/
  │   │   │   ├── city=Los_Angeles/
  │   │   │   │   ├── level=paid/
  │   │   │   │   │   ├── gender=M/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   ├── gender=F/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   └── ...
  │   │   │   │   ├── level=free/
  │   │   │   │   │   ├── gender=M/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   ├── gender=F/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   └── ...
  │   │   │   │   └── ...
  │   │   │   └── ...
  │   │   └── ...
  │   └── ...
  ├── date=2023-07-02/
  │   ├── country=US/
  │   │   ├── state=CA/
  │   │   │   ├── city=San_Francisco/
  │   │   │   │   ├── level=paid/
  │   │   │   │   │   ├── gender=M/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   ├── gender=F/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   └── ...
  │   │   │   │   ├── level=free/
  │   │   │   │   │   ├── gender=M/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   ├── gender=F/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   └── ...
  │   │   │   │   └── ...
  │   │   │   └── ...
  │   │   └── ...
  │   └── ...
  └── ...
```

It is really important to use partition, it improved query performance partitioning and it allows more efficient and faster query execution. By organizing data into partitions based on specific columns, Athena can skip scanning unnecessary data and only process the relevant partitions for a given query. This significantly reduces the amount of data that needs to be scanned, resulting in faster query response times.Cost optimization is one of the most important advantages. With partitioning, we can limit the amount of data scanned by queries, which directly impacts the cost of your Athena queries.

8. SNS topics to notify the BI team of the availability of new data in the staging area.
9. The BI team can then connect to the Athena (data warehouse) to retrieve SQL databases using the transformed data for analysis.

Lastly, once the information is available in the datalake and with the well-defined data catalog, we will be able to execute queries that allow us to find insights in the information and generate value in the organization. As an example, 4 questions are asked and answered based on the information generated from Eventsim.

• What are the locations that used more the website?

```sql
SELECT city, COUNT(*) AS total_visits
FROM "stream-stage-table-stage"
GROUP BY city
ORDER BY total_visits DESC;
```

• What are the most listened to songs in a period of time?

 
```sql
SELECT song, COUNT(*) AS listen_count
FROM "stream-stage-table-stage"
WHERE report_date >= date_parse('2022-05-01', '%Y-%m-%d') AND report_date <= date_parse('2022-06-05', '%Y-%m-%d')
GROUP BY song
ORDER BY listen_count DESC;
```

• How many hours of music a user consumes in a period of time?

```sql
SELECT userid, firstName, SUM(duration)/3600 AS total_listening_hours
FROM "stream-stage-table-stage"
WHERE report_date >= date_parse('2022-05-01', '%Y-%m-%d') AND report_date <= date_parse('2022-06-30', '%Y-%m-%d')
GROUP BY userid, firstName
ORDER BY total_listening_hours DESC;
```

• What are the two most listened to songs per person in a period of time?

```sql
SELECT userid, full_name, duration, song_user_rankinkg
FROM(
    SELECT userid, CONCAT(firstName, ' ',lastname) as full_name, duration, row_number() over (PARTITION BY userid ORDER BY duration DESC) AS song_user_rankinkg
    FROM "stream-stage-table-stage"
    WHERE report_date >= date_parse('2022-05-01', '%Y-%m-%d') AND report_date <= date_parse('2022-06-30', '%Y-%m-%d'))
WHERE song_user_rankinkg <= 2
```

### Data Model

For data modeling, the technology or service that was going to be used had to be taken into account, since generating a model for a serverless database (Athena) can be vastly different from one based on servers such as Redshift. While serverless tries to exploit the way of storing data in objects and their prefix structure that allows them to filter on them, servers are optimized to work better with indexes and normalized relational models that allow them to easily navigate their object structures. data such as hash tables and b-trees

In the case of the server technologies, the design of the data warehouse I thought it was based on the Kimbal model due to its simplicity and ease of understanding, the optimized performance for queries and its orientation towards the end user, since this is focused on the delivery of practical and relevant information for decision making. decisions, which is a key aspect in a BI area.

The star schema offers a simpler and more denormalized structure, while the snowflake schema provides a more normalized and complex structure with improved data integrity and potentially better space efficiency. The choice between the two depends on factors such as the complexity of the data, query performance requirements, and the trade-off between simplicity and normalization.

In a data warehouse, the common types of fact tables are Transactional, Fact Table, Accumulative Fact Table, Snapshot Fact Table , Event Fact Table. The choice of which type of table to use depends on the nature of the information and the intent of the business questions to be answered. For example, the cumulative time per song fact table would make it easier for us to answer the question of what is the most listened to song in a period of time.

These are some of the common types of fact tables in a data warehouse. The choice of fact table type depends on the nature of the stored data and specific analytical requirements of the business.

![Logo de Markdown](images/Datawarehouse.png)

For the design of the serverless database, which was the model that was chosen to develop this project, its design was thought about hierarchies and the way to group the data that would best allow filtering, so that when executing a query it is filtered. as much information as possible. The structure that was designed was the following:

```python
s3://my-music-data/music_data_partitioned/
  ├── date=2023-07-01/
  │   ├── country=US/
  │   │   ├── state=CA/
  │   │   │   ├── city=Los_Angeles/
  │   │   │   │   ├── level=paid/
  │   │   │   │   │   ├── gender=M/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   ├── gender=F/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   └── ...
  │   │   │   │   ├── level=free/
  │   │   │   │   │   ├── gender=M/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   ├── gender=F/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   └── ...
  │   │   │   │   └── ...
  │   │   │   └── ...
  │   │   └── ...
  │   └── ...
  ├── date=2023-07-02/
  │   ├── country=US/
  │   │   ├── state=CA/
  │   │   │   ├── city=San_Francisco/
  │   │   │   │   ├── level=paid/
  │   │   │   │   │   ├── gender=M/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   ├── gender=F/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   └── ...
  │   │   │   │   ├── level=free/
  │   │   │   │   │   ├── gender=M/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   ├── gender=F/
  │   │   │   │   │   │   ├── part-00000.parquet
  │   │   │   │   │   │   ├── part-00001.parquet
  │   │   │   │   │   │   └── ...
  │   │   │   │   │   └── ...
  │   │   │   │   └── ...
  │   │   │   └── ...
  │   │   └── ...
  │   └── ...
  └── ...
```

* [CLion](https://docs.aws.amazon.com/toolkit-for-jetbrains/latest/

## Deploy the sample application

The first to execute the pipeline is the generation of the data, this, as already mentioned, is done with Eventsim and you can use the different configurations in the "entrypoint.sh" file to simulate the environment that best suits your needs. of representation. The following code offers different alternatives:

```bash
#! /bin/bash
mc alias set minio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}
mc mb minio/de-streaming-test
nohup bash eventsim.sh -c "examples/example-config.json" --from 365 --nusers 1000 --growth-rate 0.01 ./output > eventsim.stoud & # 1 Option
nohup bash eventsim.sh -c "examples/example-config.json" --from 365 --nusers 30000 --growth-rate 0.3 ./output > eventsim.stoud & # 2 Option
# num_threads=5  # Number of threads to create
# 3 Option
for ((i=0; i<num_threads; i++))
do
    nohup bash eventsim.sh -c "examples/example-config.json" --from 365 --nusers 1000 --growth-rate 0.01 ./output > eventsim.stoud &
    nohup bash eventsim.sh -c "examples/example-config.json" --from 365 --nusers 30000 --growth-rate 0.3 ./output > eventsim.stoud &
#done
# wait # Wait for all threads to finish
fluentd -c ./fluent.conf
```

After configuring Eventsim, it generates the simulated data and it is delivered to Fluentd, which will then deliver the data to AWS Kinesis.

In AWS, a pipeline was designed that has an event configured with the EventBridge service, so that every 10 minutes it monitors if there are changes in the folders of the S3 bucket with the name "music-stream-datalake", from where all the processing begins of data from the automatic execution of the state machine configured in StepFunctions, this executes the Crawlers, one to identify the schema of the raw data and another for the data transformed by the glue job.

To deploy the cloud architecture, you must execute the following steps:


```bash
aws cloudformation package --template-file template.yaml --s3-bucket aws-cloud-formation-artifacts --output-template-file packaged.template --profile default                                          
```

```bash
aws cloudformation deploy --stack-name door2door-stack --template-file packaged.template --parameter-overrides file://parameter.json --capabilities CAPABILITY_IAM --profile default --region us-east-1                                       
```

To execute the CloudFormation template and deploy the architecture using the AWS Command Line Interface (CLI), you can follow these steps:

This deploys the entire infrastructure via cloudformation and will automatically update the information in the data lake and data warehouse.


* **Stack Name**: The name of the stack to deploy to CloudFormation. This should be unique to your account and region, and a good starting point would be something matching your project name.
* **AWS Region**: The AWS region you want to deploy your app to.
* **Confirm changes before deploy**: If set to yes, any change sets will be shown to you before execution for manual review. If set to no, the AWS SAM CLI will automatically deploy application changes.
* **Allow SAM CLI IAM role creation**: Many AWS SAM templates, including this example, create AWS IAM roles required for the AWS Lambda function(s) included to access AWS services. By default, these are scoped down to minimum required permissions. To deploy an AWS CloudFormation stack which creates or modifies IAM roles, the `CAPABILITY_IAM` value for `capabilities` must be provided. If permission isn't provided through this prompt, to deploy this example you must explicitly pass `--capabilities CAPABILITY_IAM` to the `sam deploy` command.
* **Save arguments to samconfig.toml**: If set to yes, your choices will be saved to a configuration file inside the project, so that in the future you can just re-run `sam deploy` without parameters to deploy changes to your application.

You can find your API Gateway Endpoint URL in the output values displayed after deployment.


## Cleanup

To delete the sample application that you created, use the AWS CLI. Assuming you used your project name for the stack name, you can run the following:

```bash
aws cloudformation delete-stack --stack-name stream-music-logs
```

### Future Work
The work was divided into two folders, each with a different solution to the problem. The solution that was carried out to fulfill the purposes of this project is inside the "stream-music-logs-athena" folder, but it was possible to work on an alternative solution that will be briefly explained below.

The first application focused on the use of AWS Kinesis, on the use of Glue to perform the ETL tasks and AWS Athena to simulate the DataWarehouse. But an alternative version can be proposed where instead of using AWS Kinesis, AWS Fargate is used to ingest and transmit the information, using custom lambdas within a custom module codified with the Factory design pattern, so that new ETL functions can be included, and finally make use of Redshift instead of using Athena. The design to deploy the solution with Athena was already described in the data model definition. It seeks to build an OLAP relational database, with the purpose of improving and accelerating the data analysis process in a scalable, reliable, and fault-tolerant manner.

##### Dead Letter Q

Additionally, it was also thought of developing a Dead Letter Q (DLQ), so that there is greater control and traceability of the records that enter the data pipeline, in order to mitigate the risk of loss of records without knowing it.

The Dead Letter Queue (DLQ) is a message queue used in the ETL pipeline to handle messages that cannot be processed successfully. In this case, it is used to manage trigger requests generated when files are placed in an Amazon S3 bucket.
 
The objective of the DLQ is to temporarily store undeliverable messages, providing a mechanism for further handling and debugging of processing issues.
Operation
When a file is placed in the S3 bucket, a trigger request is generated and sent to the ETL pipeline. If the trigger request cannot be processed successfully due to errors or exceptions, it is sent to the DLQ instead of being discarded entirely.

###### Configuration
Configuring the DLQ involves the following steps:
1. Create a dedicated message queue for the DLQ in the messaging service used in the pipeline (e.g., Amazon SQS).
2. Configure the ETL pipeline to send unprocessable trigger requests to the DLQ in case of errors or exceptions during processing.

###### Monitoring and Management
Monitoring and managing the DLQ include the following:

1. Monitor the number of messages in the DLQ to identify potential issues or bottlenecks in processing.
Set up alerts or notifications to receive notifications when the DLQ has a high number of pending messages.

2. Implement mechanisms to handle and process the messages in the DLQ appropriately. This may include:
  * Identifying and resolving the issues that caused the non-delivery.
  * Analyzing and debugging the messages to address recurring problems.
  * Retrying the processing of failed messages.

## Resources

See the [AWS SAM developer guide](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html) for an introduction to SAM specification, the SAM CLI, and serverless application concepts.

Next, you can use AWS Serverless Application Repository to deploy ready to use Apps that go beyond hello world samples and learn how authors developed their applications: [AWS Serverless Application Repository main page](https://aws.amazon.com/serverless/serverlessrepo/)
